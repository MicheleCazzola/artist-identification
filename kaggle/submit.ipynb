{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":85769,"databundleVersionId":9709110,"sourceType":"competition"},{"sourceId":217400,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":185379,"modelId":207517}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://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@github.com/MicheleCazzola/mvlm-project.git mlvm-project\n\n!cd mlvm-project; git status","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T23:25:34.945615Z","iopub.execute_input":"2025-01-02T23:25:34.945898Z","iopub.status.idle":"2025-01-02T23:25:37.517383Z","shell.execute_reply.started":"2025-01-02T23:25:34.945877Z","shell.execute_reply":"2025-01-02T23:25:37.516498Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'mlvm-project'...\nremote: Enumerating objects: 1050, done.\u001b[K\nremote: Counting objects: 100% (613/613), done.\u001b[K\nremote: Compressing objects: 100% (426/426), done.\u001b[K\nremote: Total 1050 (delta 319), reused 465 (delta 174), pack-reused 437 (from 1)\u001b[K\nReceiving objects: 100% (1050/1050), 7.32 MiB | 27.76 MiB/s, done.\nResolving deltas: 100% (576/576), done.\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!cp -r mlvm-project/src .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T23:25:54.376449Z","iopub.execute_input":"2025-01-02T23:25:54.376754Z","iopub.status.idle":"2025-01-02T23:25:54.506589Z","shell.execute_reply.started":"2025-01-02T23:25:54.376725Z","shell.execute_reply":"2025-01-02T23:25:54.505531Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nimport csv\nfrom src.model.network import MultiBranchArtistNetwork\nfrom torchvision import transforms\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T23:27:49.532660Z","iopub.execute_input":"2025-01-02T23:27:49.532953Z","iopub.status.idle":"2025-01-02T23:27:49.536979Z","shell.execute_reply.started":"2025-01-02T23:27:49.532930Z","shell.execute_reply":"2025-01-02T23:27:49.536102Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def preprocess_image(image_path, size, stats):\n    imagenet = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n    transform = transforms.Compose([\n        transforms.Resize((size, size)),\n        transforms.CenterCrop((size, size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=stats[0], std=stats[1]),\n    ])\n    image = Image.open(image_path).convert('RGB')\n    return transform(image).unsqueeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T23:26:03.557181Z","iopub.execute_input":"2025-01-02T23:26:03.557633Z","iopub.status.idle":"2025-01-02T23:26:03.563839Z","shell.execute_reply.started":"2025-01-02T23:26:03.557604Z","shell.execute_reply":"2025-01-02T23:26:03.563082Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def predict_image(model, image_tensor, class_names):\n    with torch.no_grad():\n        outputs = model(image_tensor)\n        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n        top5_prob, top5_catid = torch.topk(probabilities, 5)\n        return [(class_names[idx], prob.item()) for idx, prob in zip(top5_catid, top5_prob)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T23:26:05.500602Z","iopub.execute_input":"2025-01-02T23:26:05.500939Z","iopub.status.idle":"2025-01-02T23:26:05.505583Z","shell.execute_reply.started":"2025-01-02T23:26:05.500911Z","shell.execute_reply":"2025-01-02T23:26:05.504636Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def evaluate_model(model, test_dir, class_names, input_size, norm_stats, device):\n    model.eval()\n    image_files = [f for f in os.listdir(test_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\n    csv_path = '/kaggle/working/predictions.csv'\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Image Name', 'Class1', 'Class2', 'Class3', 'Class4', 'Class5'])\n\n        total = len(image_files)\n        for (step, image_file) in enumerate(image_files):\n            image_path = os.path.join(test_dir, image_file)\n            image_tensor = preprocess_image(image_path, input_size, norm_stats).to(device)\n            predictions = predict_image(model, image_tensor, class_names)\n            \n            writer.writerow([image_file] + [class_name for class_name, _ in predictions])\n\n            if (step + 1) % 100 == 0:\n                print(f\"Done step {step+1}/{total}\")\n\n    print(f\"Predictions saved to {csv_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T23:33:37.140702Z","iopub.execute_input":"2025-01-02T23:33:37.141028Z","iopub.status.idle":"2025-01-02T23:33:37.147970Z","shell.execute_reply.started":"2025-01-02T23:33:37.141005Z","shell.execute_reply":"2025-01-02T23:33:37.147188Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"train_dir = \"/kaggle/input/artist-identification/artist_dataset/train\"\ntest_dir = \"/kaggle/input/artist-identification/artist_dataset/test\"\n\nNUM_CLASSES = 161\nINPUT_SIZE = 512\nNORM_STATS = [0.4748,0.4197,0.3591], [0.2792, 0.2740,0.2634]\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T23:32:45.908609Z","iopub.execute_input":"2025-01-02T23:32:45.908942Z","iopub.status.idle":"2025-01-02T23:32:45.913360Z","shell.execute_reply.started":"2025-01-02T23:32:45.908917Z","shell.execute_reply":"2025-01-02T23:32:45.912295Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"model = MultiBranchArtistNetwork(num_classes=NUM_CLASSES, use_handcrafted=False).to(DEVICE)\nmodel.load_state_dict(torch.load(\"/kaggle/input/random_b32_21epochs_wd1e-6_noaug/pytorch/default/1/best_model_10.pth\", weights_only=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T23:32:48.424888Z","iopub.execute_input":"2025-01-02T23:32:48.425233Z","iopub.status.idle":"2025-01-02T23:32:48.866369Z","shell.execute_reply.started":"2025-01-02T23:32:48.425202Z","shell.execute_reply":"2025-01-02T23:32:48.865629Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"class_names = os.listdir(train_dir)\nassert len(class_names) == NUM_CLASSES","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T23:26:50.209007Z","iopub.execute_input":"2025-01-02T23:26:50.209366Z","iopub.status.idle":"2025-01-02T23:26:50.213680Z","shell.execute_reply.started":"2025-01-02T23:26:50.209339Z","shell.execute_reply":"2025-01-02T23:26:50.213048Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"evaluate_model(model, test_dir, class_names, INPUT_SIZE, NORM_STATS, DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T23:33:41.460101Z","iopub.execute_input":"2025-01-02T23:33:41.460456Z","iopub.status.idle":"2025-01-02T23:37:12.220869Z","shell.execute_reply.started":"2025-01-02T23:33:41.460431Z","shell.execute_reply":"2025-01-02T23:37:12.220052Z"}},"outputs":[{"name":"stdout","text":"Done step 100/3960\nDone step 200/3960\nDone step 300/3960\nDone step 400/3960\nDone step 500/3960\nDone step 600/3960\nDone step 700/3960\nDone step 800/3960\nDone step 900/3960\nDone step 1000/3960\nDone step 1100/3960\nDone step 1200/3960\nDone step 1300/3960\nDone step 1400/3960\nDone step 1500/3960\nDone step 1600/3960\nDone step 1700/3960\nDone step 1800/3960\nDone step 1900/3960\nDone step 2000/3960\nDone step 2100/3960\nDone step 2200/3960\nDone step 2300/3960\nDone step 2400/3960\nDone step 2500/3960\nDone step 2600/3960\nDone step 2700/3960\nDone step 2800/3960\nDone step 2900/3960\nDone step 3000/3960\nDone step 3100/3960\nDone step 3200/3960\nDone step 3300/3960\nDone step 3400/3960\nDone step 3500/3960\nDone step 3600/3960\nDone step 3700/3960\nDone step 3800/3960\nDone step 3900/3960\nPredictions saved to /kaggle/working/predictions.csv\n","output_type":"stream"}],"execution_count":31}]}